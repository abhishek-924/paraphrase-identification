"""# MALSTM MODEL"""

class Manhattan_LSTM(nn.Module):
    def __init__(self, hidden_size, embedding, train_embedding = False):
        super(Manhattan_LSTM, self).__init__()
        self.use_cuda = torch.cuda.is_available()
        self.hidden_size = hidden_size
        
        self.embedding = nn.Embedding(embedding.shape[0], embedding.shape[1])
        self.embedding.weight = nn.Parameter(embedding)
        self.input_size = embedding.shape[1]
        
        self.embedding.weight.requires_grad = train_embedding
        
        self.lstm_1 = nn.LSTM(self.input_size, self.hidden_size, num_layers=1, bidirectional=True)
        self.lstm_2 = nn.LSTM(self.input_size, self.hidden_size, num_layers=1, bidirectional=True)
        
    def exponent_neg_manhattan_distance(self, x1, x2):
        return torch.exp(-torch.sum(torch.abs(x1 - x2), dim=1))
    
    def forward(self, input, hidden):
        
        #print(input[0])
        #print(input[1])
        
        ip0 = input[0].t()
        ip1 = input[1].t()
        
        commonList = []
        
        for i in range(batch_size):
            listPairs = commonWords(ip0[i], ip1[i])
            commonList.append(listPairs)
    
        commonList = np.array(commonList)
        
        #print(commonList)
        input_len = len(input[1])
        
        embedded_1 = self.embedding(input[0])
        embedded_2 = self.embedding(input[1])
        
        bs = embedded_1.size()[1]
        outputs_1, hidden_1 = self.lstm_1(embedded_1, hidden)
        outputs_2, hidden_2 = self.lstm_1(embedded_2, hidden)
        
        max_pool_1 = F.adaptive_max_pool1d(outputs_1.permute(1,2,0),1).view(batch_size,-1)
        max_pool_2 = F.adaptive_max_pool1d(outputs_2.permute(1,2,0),1).view(batch_size,-1)
        
        att_weights = torch.bmm(max_pool_1.view(batch_size, 1, 100), outputs_2.view(batch_size, 100, input_len)).view(batch_size, input_len)
        
        att_softmax = torch.zeros([batch_size, input_len])
        for i in range(batch_size):
          att_softmax[i] = F.softmax(att_weights[i], dim = 0)
        
        new_pool = torch.bmm(att_softmax.view(batch_size, 1, input_len), outputs_2.view(batch_size, input_len, 100).cpu()).view(batch_size, 100).cuda()
        
        ehs_1 = []
        for i in range(batch_size):
            e_list = []
            for j in range(len(commonList[i][0])):
                x = commonList[i][0][j]
              
                e_list.append(outputs_1[x][i])
            if len(e_list) > 0:
                mp1 = max_pool(e_list)
            else:
                mp1 = torch.zeros(100)
              
            ehs_1.append(mp1.cuda())
        
        
        ehs_2 = []
        for i in range(batch_size):
            e_list = []
            for j in range(len(commonList[i][1])):
                x = commonList[i][1][j]
              
                e_list.append(outputs_2[x][i])
            if len(e_list) > 0:
                mp2 = max_pool(e_list)
            else:
                mp2 = torch.zeros(100)
              
            ehs_2.append(mp2.cuda())
            
        elitehs_1 = torch.zeros(batch_size, 100)
        for i in range(batch_size):
            elitehs_1[i] = ehs_1[i]
          
        elitehs_2 = torch.zeros(batch_size, 100)
        for i in range(batch_size):
            elitehs_2[i] = ehs_2[i]
        
        elitehs_1.cuda()
        elitehs_2.cuda()
        #similarity_scores = self.exponent_neg_manhattan_distance(ths_1.cuda(), ths_2.cuda())
        similarity_scores = self.exponent_neg_manhattan_distance(max_pool_1, new_pool)
        
        return similarity_scores, elitehs_1, elitehs_2
    
